{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/hdakhli/mlops-2024/blob/main/30_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d0cbbae3e38c51a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\n",
    "## Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828a0b4f0962e22e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "\n",
    "# Download EBOOK ALICE'S ADVENTURES IN WONDERLAND\n",
    "# https://www.gutenberg.org/files/11/11-0.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf1fa2a7ea0c535"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lire le fichier 'alice.txt'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ae46de51f1ebbeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remplacer les retours à la ligne \\n par des espaces"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd3cc866a569d66c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extraire chaque phrase et les mots qui la compose en un tableau de tableau à l'aide des fonctions sent_tokenize et word_tokenize de nltk.tokenize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d2e05ebc1405b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Créer 2 modèles: CBOW et Skip Gram à partir de ce tableau. cf doc: gensim.models.Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a2e8606f3e9d682"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Comparer les mots suivants avec les 2 modelès : \n",
    "# 'alice' vs 'wonderland'\n",
    "# 'alice' vs 'machines'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3376a3d736522423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f9e71f777abcef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2eacf360bd9f38f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b6131519c8b43939"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\n",
    "#\n",
    "## LLM\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e04b08c39eea43bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download: https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00cb9390a0d80f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Put the location of to the GGUF model that you've download from HuggingFace here\n",
    "model_path = \"llama-2-7b-chat.Q2_K.gguf\"\n",
    "\n",
    "# Create a model\n",
    "model = Llama(model_path=model_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b3a74457e6c622"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prompt creation\n",
    "system_message = \"Vous êtes un assistant utile. Vous êtes là pour m'aider avec mes questions\"\n",
    "user_message = \"Quelle est la capitale de la France ?\"\n",
    "\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "# Model parameters\n",
    "max_tokens = 50\n",
    "\n",
    "# Run the model\n",
    "output = model(prompt, max_tokens=max_tokens, echo=True)\n",
    "\n",
    "# Print the model output\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ff6118b6cf97bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_message = \"Et celle de l'Espagne ? \"\n",
    "prompt = f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "output = model(prompt, max_tokens=max_tokens, echo=True)\n",
    "\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddf6b2c73feeb2a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Que remarquez-vous ?\n",
    "# Comment pourriez-vous résoudre le problème ?\n",
    "# Exposer ce model en API REST avec FastAPI et Uvicorn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ad75aa06671e005"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a95f47d91247b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langchain-core langchain-community"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4780bd8a11d43fc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q2_K.gguf\", verbose=False\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Vous êtes un assistant utile. Vous êtes là pour répondre à mes questions {input}.\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7283cb2e56a36ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain.invoke({\"Quelle est la capitale de la France ?\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32cc8471f2694870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chain.invoke({\"Et celle de l'Espagne ?\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "721a389708180ae9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Faites en sorte que le modèle prenne en compte l'historique de la conversation\n",
    "# Exposer le model en API à l'aide de fast api et langserve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a06edee44fd2f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
